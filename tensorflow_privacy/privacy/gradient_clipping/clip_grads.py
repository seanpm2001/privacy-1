# Copyright 2022, The TensorFlow Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Computes per-example loss clip weights.

For a given Keras model and batch of inputs, computes the per-example
clip weights so that the gradient of the loss function, weighted by these
weights, is equivalent to the gradient of the original loss function but
with the per-example gradients clipped by some clip weight.
"""

import tensorflow as tf
from tensorflow_privacy.privacy.gradient_clipping import utils


def compute_gradient_norms(input_model, x_batch, y_batch, layer_registry):
  """Computes the per-example loss gradient norms for given data.

  Applies the approach given in https://arxiv.org/pdf/2009.03106.pdf, except
  the batch matrix multiplication operation in Algorithm 2 is replaced with
  the computation of two norm computations.

  Args:
    input_model: The Keras model to obtain the layers from.
    x_batch: A Tensor representing a batch of inputs to the model. The first
      axis should be the batch dimension.
    y_batch: A Tensor representing a batch of output labels. The first axis
      should be the batch dimension. The number of examples should match the
      number of examples in x_batch.
    layer_registry: A dictionary of layers that support "fast" gradient norm
      computations. The key is the class of the layer and the value is a
      function that returns a triple (output, sqr_grad_norms, vars), where
      output is the pre-activator tensor, sqr_grad_norms is the square of the
      norm of the layer's input, and vars is an ordered list of the trainable
      weights.

  Returns:
    A 1D Tensor whose i-th entry is the norm of the gradient of the i-th
    per-example loss function.
  """
  tape = tf.GradientTape(persistent=True, watch_accessed_variables=False)
  # First loop computes the norms of the layer inputs, caches these inputs,
  # and computes the summed loss.
  with tape:
    model_outputs, pre_norm_list, var_list = utils.forward_norm_pass(
        input_model, x_batch, tape, layer_registry)
    loss_config = input_model.loss.get_config()
    loss_config['reduction'] = tf.keras.losses.Reduction.NONE
    per_example_loss_fn = input_model.loss.from_config(loss_config)
    losses = per_example_loss_fn(y_batch, model_outputs)
    summed_loss = tf.reduce_sum(losses)
  # Second loop computes the norm of the gradient of the loss with respect to
  # the pre-activation tensors, and multiplies these norms with the results of
  # the first loop.
  full_norm_list = []
  for i in range(len(var_list)):
    post_sqr_grads = tf.square(tape.gradient(summed_loss, var_list[i]))
    reduction_axes = tf.range(1, tf.rank(post_sqr_grads))
    post_norm = tf.reduce_sum(post_sqr_grads, axis=reduction_axes)
    full_norm_list.append(post_norm * pre_norm_list[i])
  del tape
  # Post-processing for compatibility with non-eager mode (very annoying).
  full_norm_tsr = tf.stack(full_norm_list, axis=1)
  return tf.sqrt(tf.reduce_sum(full_norm_tsr, axis=1))


def compute_clip_weights(clip_value, gradient_norms):
  """Computes the per-example loss/clip weights for clipping.

  When the sum of the per-example losses is replaced a weighted sum, where
  the weights are generated by this method, then the gradients of each
  term in the weighted sum are clipped by the given clip value.

  Args:
    clip_value: A double representing an upper bound on the gradient norms. That
      is, all gradients of the per-example loss functions will have norm at most
      clip_value.
    gradient_norms: A 1D tensor whose i-th entry is the norm of the gradient of
      the loss function for the i-th input.

  Returns:
    A 1D Tensor representing whose i-th entry C[i] is either 1.0 if the norm
    of the gradient of i-th per-example loss G[i] is less than clip_value and
    a number less than 1.0 so that |G[i]| * C[i] == clip_norm otherwise.
  """
  if clip_value is None:
    return None
  return clip_value / tf.math.maximum(clip_value, gradient_norms)


def compute_pred_and_clipped_gradients(input_model, x_batch, y_batch,
                                       sample_weights, clip_value,
                                       layer_registry):
  """Computes the per-example predictions and per-example clipped loss gradient.

  Args:
    input_model: The Keras model to obtain the layers from.
    x_batch: A Tensor representing a batch of inputs to the model. The first
      axis should be the batch dimension.
    y_batch: A Tensor representing a batch of output labels. The first axis
      should be the batch dimension. The number of examples should match the
      number of examples in x_batch.
    sample_weights: A Tensor representing the batch sample weights of the model.
      The first axis should be the batch dimension.
    clip_value: A double representing an upper bound on the gradient norms. That
      is, all gradients of the per-example loss functions will have norm at most
      clip_value.
    layer_registry: A dictionary of layers that support "fast" gradient norm
      computations. The key is the class of the layer and the value is a
      function that returns a triple (output, sqr_grad_norms, vars), where
      output is the pre-activator tensor, sqr_grad_norms is the square of the
      norm of the layer's input, and vars is an ordered list of the trainable
      weights.

  Returns:
    A pair (y_pred, grad). The first element is the prediction generated by
    the model on the input x_batch. The element is the clipped and
    sampled-weighted gradient of the loss function.
  """
  gradient_norms = compute_gradient_norms(input_model, x_batch, y_batch,
                                          layer_registry)
  clip_weights = compute_clip_weights(clip_value, gradient_norms)
  loss_weights = None
  if sample_weights is not None and clip_weights is not None:
    loss_weights = clip_weights * sample_weights
  elif sample_weights is not None:
    loss_weights = sample_weights
  elif clip_weights is not None:
    loss_weights = clip_weights
  with tf.GradientTape() as tape:
    y_pred = input_model(x_batch, training=True)
    loss_value = input_model.compute_loss(x_batch, y_batch, y_pred,
                                          loss_weights)
  return y_pred, tape.gradient(loss_value, input_model.trainable_variables)
